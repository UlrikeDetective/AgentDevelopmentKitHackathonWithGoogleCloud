{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a72da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation complete.\n"
     ]
    }
   ],
   "source": [
    "# @title Step 0: Setup and Installation\n",
    "# Install ADK and LiteLLM for multi-model support\n",
    "\n",
    "\n",
    "!pip install google-adk -q\n",
    "!pip install litellm -q\n",
    "\n",
    "print(\"Installation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40712c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# @title Import necessary libraries\n",
    "import os\n",
    "import asyncio\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b03c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys Set:\n",
      "Google API Key set: Yes\n"
     ]
    }
   ],
   "source": [
    "# @title Configure API Keys (Load from .env file)\n",
    "\n",
    "# Install python-dotenv if not already installed\n",
    "!pip install python-dotenv -q\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Gemini API Key (loaded from .env)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY if GOOGLE_API_KEY else \"\"\n",
    "\n",
    "# [Optional]\n",
    "# OpenAI API Key (Get from OpenAI Platform: https://platform.openai.com/api-keys)\n",
    "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "# os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY if OPENAI_API_KEY else \"\"\n",
    "\n",
    "# [Optional]\n",
    "# Anthropic API Key (Get from Anthropic Console: https://console.anthropic.com/settings/keys)\n",
    "# ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "# os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY if ANTHROPIC_API_KEY else \"\"\n",
    "\n",
    "# --- Verify Keys (Optional Check) ---\n",
    "print(\"API Keys Set:\")\n",
    "print(f\"Google API Key set: {'Yes' if GOOGLE_API_KEY and GOOGLE_API_KEY != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "# print(f\"OpenAI API Key set: {'Yes' if OPENAI_API_KEY and OPENAI_API_KEY != 'YOUR_OPENAI_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "# print(f\"Anthropic API Key set: {'Yes' if ANTHROPIC_API_KEY and ANTHROPIC_API_KEY != 'YOUR_ANTHROPIC_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "\n",
    "# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
    "\n",
    "# @markdown **Security Note:** It's best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Now loading from .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f64d7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configured.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "\n",
    "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
    "\n",
    "# Note: Specific model names might change. Refer to LiteLLM/Provider documentation.\n",
    "# MODEL_GPT_4O = \"openai/gpt-4o\"\n",
    "# MODEL_CLAUDE_SONNET = \"anthropic/claude-3-sonnet-20240229\"\n",
    "\n",
    "\n",
    "print(\"\\nEnvironment configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e1fed",
   "metadata": {},
   "source": [
    "### Step 1: Your First Agent - Basic Weather Lookup¶\n",
    "Let's begin by building the fundamental component of our Weather Bot: a single agent capable of performing a specific task – looking up weather information. This involves creating two core pieces:\n",
    "\n",
    "A Tool: A Python function that equips the agent with the ability to fetch weather data.\n",
    "An Agent: The AI \"brain\" that understands the user's request, knows it has a weather tool, and decides when and how to use it.\n",
    "\n",
    "### 1. Define the Tool (get_weather)\n",
    "\n",
    "In ADK, Tools are the building blocks that give agents concrete capabilities beyond just text generation. They are typically regular Python functions that perform specific actions, like calling an API, querying a database, or performing calculations.\n",
    "\n",
    "Our first tool will provide a mock weather report. This allows us to focus on the agent structure without needing external API keys yet. Later, you could easily swap this mock function with one that calls a real weather service.\n",
    "\n",
    "Key Concept: Docstrings are Crucial! The agent's LLM relies heavily on the function's docstring to understand:\n",
    "\n",
    "What the tool does.\n",
    "When to use it.\n",
    "What arguments it requires (city: str).\n",
    "What information it returns.\n",
    "Best Practice: Write clear, descriptive, and accurate docstrings for your tools. This is essential for the LLM to use the tool correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346f07d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: get_weather called for city: New York ---\n",
      "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25°C.'}\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
     ]
    }
   ],
   "source": [
    "# @title Define the get_weather Tool\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Retrieves the current weather report for a specified city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the weather information.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'report' key with weather details.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
    "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
    "\n",
    "    # Mock weather data\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\n",
    "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15°C.\"},\n",
    "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        return mock_weather_db[city_normalized]\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
    "\n",
    "# Example tool usage (optional test)\n",
    "print(get_weather(\"New York\"))\n",
    "print(get_weather(\"Paris\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f00d0",
   "metadata": {},
   "source": [
    "### 2. Define the Agent (weather_agent)\n",
    "\n",
    "Now, let's create the Agent itself. An Agent in ADK orchestrates the interaction between the user, the LLM, and the available tools.\n",
    "\n",
    "We configure it with several key parameters:\n",
    "\n",
    "name: A unique identifier for this agent (e.g., \"weather_agent_v1\").\n",
    "model: Specifies which LLM to use (e.g., MODEL_GEMINI_2_0_FLASH). We'll start with a specific Gemini model.\n",
    "description: A concise summary of the agent's overall purpose. This becomes crucial later when other agents need to decide whether to delegate tasks to this agent.\n",
    "instruction: Detailed guidance for the LLM on how to behave, its persona, its goals, and specifically how and when to utilize its assigned tools.\n",
    "tools: A list containing the actual Python tool functions the agent is allowed to use (e.g., [get_weather]).\n",
    "Best Practice: Provide clear and specific instruction prompts. The more detailed the instructions, the better the LLM can understand its role and how to use its tools effectively. Be explicit about error handling if needed.\n",
    "\n",
    "Best Practice: Choose descriptive name and description values. These are used internally by ADK and are vital for features like automatic delegation (covered later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f20e07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash'.\n"
     ]
    }
   ],
   "source": [
    "# @title Define the Weather Agent\n",
    "# Use one of the model constants defined earlier\n",
    "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"weather_agent_v1\",\n",
    "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
    "    description=\"Provides weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use the 'get_weather' tool to find the information. \"\n",
    "                \"If the tool returns an error, inform the user politely. \"\n",
    "                \"If the tool is successful, present the weather report clearly.\",\n",
    "    tools=[get_weather], # Pass the function directly\n",
    ")\n",
    "\n",
    "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e2289",
   "metadata": {},
   "source": [
    "### 3. Setup Runner and Session Service\n",
    "\n",
    "To manage conversations and execute the agent, we need two more components:\n",
    "\n",
    "SessionService: Responsible for managing conversation history and state for different users and sessions. The InMemorySessionService is a simple implementation that stores everything in memory, suitable for testing and simple applications. It keeps track of the messages exchanged. We'll explore state persistence more in Step 4.\n",
    "Runner: The engine that orchestrates the interaction flow. It takes user input, routes it to the appropriate agent, manages calls to the LLM and tools based on the agent's logic, handles session updates via the SessionService, and yields events representing the progress of the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60569bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n",
      "Runner created for agent 'weather_agent_v1'.\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Session Service and Runner\n",
    "\n",
    "# --- Session Management ---\n",
    "# Key Concept: SessionService stores conversation history & state.\n",
    "# InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Define constants for identifying the interaction context\n",
    "APP_NAME = \"weather_tutorial_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
    "\n",
    "# Create the specific session where the conversation will happen\n",
    "session = await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID\n",
    ")\n",
    "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
    "\n",
    "# --- Runner ---\n",
    "# Key Concept: Runner orchestrates the agent execution loop.\n",
    "runner = Runner(\n",
    "    agent=weather_agent, # The agent we want to run\n",
    "    app_name=APP_NAME,   # Associates runs with our app\n",
    "    session_service=session_service # Uses our session manager\n",
    ")\n",
    "print(f\"Runner created for agent '{runner.agent.name}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
