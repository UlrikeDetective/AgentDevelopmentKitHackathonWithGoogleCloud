{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a72da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation complete.\n"
     ]
    }
   ],
   "source": [
    "# @title Step 0: Setup and Installation\n",
    "# Install ADK and LiteLLM for multi-model support\n",
    "\n",
    "\n",
    "!pip install google-adk -q\n",
    "!pip install litellm -q\n",
    "\n",
    "print(\"Installation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40712c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# @title Import necessary libraries\n",
    "import os\n",
    "import asyncio\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b03c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys Set:\n",
      "Google API Key set: Yes\n"
     ]
    }
   ],
   "source": [
    "# @title Configure API Keys (Load from .env file)\n",
    "\n",
    "# Install python-dotenv if not already installed\n",
    "!pip install python-dotenv -q\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Gemini API Key (loaded from .env)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY if GOOGLE_API_KEY else \"\"\n",
    "\n",
    "# [Optional]\n",
    "# OpenAI API Key (Get from OpenAI Platform: https://platform.openai.com/api-keys)\n",
    "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "# os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY if OPENAI_API_KEY else \"\"\n",
    "\n",
    "# [Optional]\n",
    "# Anthropic API Key (Get from Anthropic Console: https://console.anthropic.com/settings/keys)\n",
    "# ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "# os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY if ANTHROPIC_API_KEY else \"\"\n",
    "\n",
    "# --- Verify Keys (Optional Check) ---\n",
    "print(\"API Keys Set:\")\n",
    "print(f\"Google API Key set: {'Yes' if GOOGLE_API_KEY and GOOGLE_API_KEY != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "# print(f\"OpenAI API Key set: {'Yes' if OPENAI_API_KEY and OPENAI_API_KEY != 'YOUR_OPENAI_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "# print(f\"Anthropic API Key set: {'Yes' if ANTHROPIC_API_KEY and ANTHROPIC_API_KEY != 'YOUR_ANTHROPIC_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "\n",
    "# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
    "\n",
    "# @markdown **Security Note:** It's best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Now loading from .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f64d7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configured.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "\n",
    "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
    "\n",
    "# Note: Specific model names might change. Refer to LiteLLM/Provider documentation.\n",
    "# MODEL_GPT_4O = \"openai/gpt-4o\"\n",
    "# MODEL_CLAUDE_SONNET = \"anthropic/claude-3-sonnet-20240229\"\n",
    "\n",
    "\n",
    "print(\"\\nEnvironment configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e1fed",
   "metadata": {},
   "source": [
    "### Step 1: Your First Agent - Basic Weather Lookup¶\n",
    "Let's begin by building the fundamental component of our Weather Bot: a single agent capable of performing a specific task – looking up weather information. This involves creating two core pieces:\n",
    "\n",
    "A Tool: A Python function that equips the agent with the ability to fetch weather data.\n",
    "An Agent: The AI \"brain\" that understands the user's request, knows it has a weather tool, and decides when and how to use it.\n",
    "\n",
    "### 1. Define the Tool (get_weather)\n",
    "\n",
    "In ADK, Tools are the building blocks that give agents concrete capabilities beyond just text generation. They are typically regular Python functions that perform specific actions, like calling an API, querying a database, or performing calculations.\n",
    "\n",
    "Our first tool will provide a mock weather report. This allows us to focus on the agent structure without needing external API keys yet. Later, you could easily swap this mock function with one that calls a real weather service.\n",
    "\n",
    "Key Concept: Docstrings are Crucial! The agent's LLM relies heavily on the function's docstring to understand:\n",
    "\n",
    "What the tool does.\n",
    "When to use it.\n",
    "What arguments it requires (city: str).\n",
    "What information it returns.\n",
    "Best Practice: Write clear, descriptive, and accurate docstrings for your tools. This is essential for the LLM to use the tool correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346f07d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: get_weather called for city: New York ---\n",
      "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25°C.'}\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
     ]
    }
   ],
   "source": [
    "# @title Define the get_weather Tool\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Retrieves the current weather report for a specified city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the weather information.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'report' key with weather details.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
    "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
    "\n",
    "    # Mock weather data\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\n",
    "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15°C.\"},\n",
    "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        return mock_weather_db[city_normalized]\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
    "\n",
    "# Example tool usage (optional test)\n",
    "print(get_weather(\"New York\"))\n",
    "print(get_weather(\"Paris\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f00d0",
   "metadata": {},
   "source": [
    "### 2. Define the Agent (weather_agent)\n",
    "\n",
    "Now, let's create the Agent itself. An Agent in ADK orchestrates the interaction between the user, the LLM, and the available tools.\n",
    "\n",
    "We configure it with several key parameters:\n",
    "\n",
    "name: A unique identifier for this agent (e.g., \"weather_agent_v1\").\n",
    "model: Specifies which LLM to use (e.g., MODEL_GEMINI_2_0_FLASH). We'll start with a specific Gemini model.\n",
    "description: A concise summary of the agent's overall purpose. This becomes crucial later when other agents need to decide whether to delegate tasks to this agent.\n",
    "instruction: Detailed guidance for the LLM on how to behave, its persona, its goals, and specifically how and when to utilize its assigned tools.\n",
    "tools: A list containing the actual Python tool functions the agent is allowed to use (e.g., [get_weather]).\n",
    "Best Practice: Provide clear and specific instruction prompts. The more detailed the instructions, the better the LLM can understand its role and how to use its tools effectively. Be explicit about error handling if needed.\n",
    "\n",
    "Best Practice: Choose descriptive name and description values. These are used internally by ADK and are vital for features like automatic delegation (covered later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f20e07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash'.\n"
     ]
    }
   ],
   "source": [
    "# @title Define the Weather Agent\n",
    "# Use one of the model constants defined earlier\n",
    "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"weather_agent_v1\",\n",
    "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
    "    description=\"Provides weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use the 'get_weather' tool to find the information. \"\n",
    "                \"If the tool returns an error, inform the user politely. \"\n",
    "                \"If the tool is successful, present the weather report clearly.\",\n",
    "    tools=[get_weather], # Pass the function directly\n",
    ")\n",
    "\n",
    "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e2289",
   "metadata": {},
   "source": [
    "### 3. Setup Runner and Session Service\n",
    "\n",
    "To manage conversations and execute the agent, we need two more components:\n",
    "\n",
    "SessionService: Responsible for managing conversation history and state for different users and sessions. The InMemorySessionService is a simple implementation that stores everything in memory, suitable for testing and simple applications. It keeps track of the messages exchanged. We'll explore state persistence more in Step 4.\n",
    "Runner: The engine that orchestrates the interaction flow. It takes user input, routes it to the appropriate agent, manages calls to the LLM and tools based on the agent's logic, handles session updates via the SessionService, and yields events representing the progress of the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60569bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n",
      "Runner created for agent 'weather_agent_v1'.\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Session Service and Runner\n",
    "\n",
    "# --- Session Management ---\n",
    "# Key Concept: SessionService stores conversation history & state.\n",
    "# InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Define constants for identifying the interaction context\n",
    "APP_NAME = \"weather_tutorial_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
    "\n",
    "# Create the specific session where the conversation will happen\n",
    "session = await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID\n",
    ")\n",
    "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
    "\n",
    "# --- Runner ---\n",
    "# Key Concept: Runner orchestrates the agent execution loop.\n",
    "runner = Runner(\n",
    "    agent=weather_agent, # The agent we want to run\n",
    "    app_name=APP_NAME,   # Associates runs with our app\n",
    "    session_service=session_service # Uses our session manager\n",
    ")\n",
    "print(f\"Runner created for agent '{runner.agent.name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac58b3",
   "metadata": {},
   "source": [
    "### 4. Interact with the Agent\n",
    "\n",
    "We need a way to send messages to our agent and receive its responses. Since LLM calls and tool executions can take time, ADK's Runner operates asynchronously.\n",
    "\n",
    "We'll define an async helper function (call_agent_async) that:\n",
    "\n",
    "Takes a user query string.\n",
    "Packages it into the ADK Content format.\n",
    "Calls runner.run_async, providing the user/session context and the new message.\n",
    "Iterates through the Events yielded by the runner. Events represent steps in the agent's execution (e.g., tool call requested, tool result received, intermediate LLM thought, final response).\n",
    "Identifies and prints the final response event using event.is_final_response().\n",
    "Why async? Interactions with LLMs and potentially tools (like external APIs) are I/O-bound operations. Using asyncio allows the program to handle these operations efficiently without blocking execution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e76dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Agent Interaction Function\n",
    "\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "async def call_agent_async(query: str, runner, user_id, session_id):\n",
    "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "  print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "  # Prepare the user's message in ADK format\n",
    "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
    "\n",
    "  # Key Concept: run_async executes the agent logic and yields Events.\n",
    "  # We iterate through events to find the final answer.\n",
    "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
    "      # You can uncomment the line below to see *all* events during execution\n",
    "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "      if event.is_final_response():\n",
    "          if event.content and event.content.parts:\n",
    "             # Assuming text response in the first part\n",
    "             final_response_text = event.content.parts[0].text\n",
    "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
    "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "          # Add more checks here if needed (e.g., specific error codes)\n",
    "          break # Stop processing events once the final response is found\n",
    "\n",
    "  print(f\"<<< Agent Response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035a72b",
   "metadata": {},
   "source": [
    "### 5. Run the Conversation\n",
    "\n",
    "Finally, let's test our setup by sending a few queries to the agent. We wrap our async calls in a main async function and run it using await.\n",
    "\n",
    "Watch the output:\n",
    "\n",
    "See the user queries.\n",
    "Notice the --- Tool: get_weather called... --- logs when the agent uses the tool.\n",
    "Observe the agent's final responses, including how it handles the case where weather data isn't available (for Paris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b9cce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: What is the weather like in London?\n",
      "--- Tool: get_weather called for city: London ---\n",
      "--- Tool: get_weather called for city: London ---\n",
      "<<< Agent Response: The weather in London is cloudy with a temperature of 15°C.\n",
      "\n",
      "\n",
      ">>> User Query: How about Paris?\n",
      "<<< Agent Response: The weather in London is cloudy with a temperature of 15°C.\n",
      "\n",
      "\n",
      ">>> User Query: How about Paris?\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "<<< Agent Response: I am sorry, I don't have weather information for Paris.\n",
      "\n",
      "\n",
      ">>> User Query: Tell me the weather in New York\n",
      "<<< Agent Response: I am sorry, I don't have weather information for Paris.\n",
      "\n",
      "\n",
      ">>> User Query: Tell me the weather in New York\n",
      "--- Tool: get_weather called for city: New York ---\n",
      "--- Tool: get_weather called for city: New York ---\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
      "\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Run the Initial Conversation\n",
    "\n",
    "# We need an async function to await our interaction helper\n",
    "async def run_conversation():\n",
    "    await call_agent_async(\"What is the weather like in London?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "    await call_agent_async(\"How about Paris?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID) # Expecting the tool's error message\n",
    "\n",
    "    await call_agent_async(\"Tell me the weather in New York\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
    "await run_conversation()\n",
    "\n",
    "# --- OR ---\n",
    "\n",
    "# Uncomment the following lines if running as a standard Python script (.py file):\n",
    "# import asyncio\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         asyncio.run(run_conversation())\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652deef",
   "metadata": {},
   "source": [
    "### Step 2: Going Multi-Model with LiteLLM [Optional]¶\n",
    "In Step 1, we built a functional Weather Agent powered by a specific Gemini model. While effective, real-world applications often benefit from the flexibility to use different Large Language Models (LLMs). Why?\n",
    "\n",
    "Performance: Some models excel at specific tasks (e.g., coding, reasoning, creative writing).\n",
    "Cost: Different models have varying price points.\n",
    "Capabilities: Models offer diverse features, context window sizes, and fine-tuning options.\n",
    "Availability/Redundancy: Having alternatives ensures your application remains functional even if one provider experiences issues.\n",
    "ADK makes switching between models seamless through its integration with the LiteLLM library. LiteLLM acts as a consistent interface to over 100 different LLMs.\n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "Learn how to configure an ADK Agent to use models from providers like OpenAI (GPT) and Anthropic (Claude) using the LiteLlm wrapper.\n",
    "Define, configure (with their own sessions and runners), and immediately test instances of our Weather Agent, each backed by a different LLM.\n",
    "Interact with these different agents to observe potential variations in their responses, even when using the same underlying tool.\n",
    "\n",
    "1. Import LiteLlm\n",
    "\n",
    "We imported this during the initial setup (Step 0), but it's the key component for multi-model support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9098aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Import LiteLlm\n",
    "from google.adk.models.lite_llm import LiteLlm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab0b6b",
   "metadata": {},
   "source": [
    "#### 2. Define and Test Multi-Model Agents\n",
    "\n",
    "Instead of passing only a model name string (which defaults to Google's Gemini models), we wrap the desired model identifier string within the LiteLlm class.\n",
    "\n",
    "Key Concept: LiteLlm Wrapper: The LiteLlm(model=\"provider/model_name\") syntax tells ADK to route requests for this agent through the LiteLLM library to the specified model provider.\n",
    "Make sure you have configured the necessary API keys for OpenAI and Anthropic in Step 0. We'll use the call_agent_async function (defined earlier, which now accepts runner, user_id, and session_id) to interact with each agent immediately after its setup.\n",
    "\n",
    "Each block below will:\n",
    "\n",
    "Define the agent using a specific LiteLLM model (MODEL_GPT_4O or MODEL_CLAUDE_SONNET).\n",
    "Create a new, separate InMemorySessionService and session specifically for that agent's test run. This keeps the conversation histories isolated for this demonstration.\n",
    "Create a Runner configured for the specific agent and its session service.\n",
    "Immediately call call_agent_async to send a query and test the agent.\n",
    "Best Practice: Use constants for model names (like MODEL_GPT_4O, MODEL_CLAUDE_SONNET defined in Step 0) to avoid typos and make code easier to manage.\n",
    "\n",
    "Error Handling: We wrap the agent definitions in try...except blocks. This prevents the entire code cell from failing if an API key for a specific provider is missing or invalid, allowing the tutorial to proceed with the models that are configured.\n",
    "\n",
    "First, let's create and test the agent using OpenAI's GPT-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define and Test GPT Agent\n",
    "\n",
    "# Make sure 'get_weather' function from Step 1 is defined in your environment.\n",
    "# Make sure 'call_agent_async' is defined from earlier.\n",
    "\n",
    "# --- Agent using GPT-4o ---\n",
    "weather_agent_gpt = None # Initialize to None\n",
    "runner_gpt = None      # Initialize runner to None\n",
    "\n",
    "try:\n",
    "    weather_agent_gpt = Agent(\n",
    "        name=\"weather_agent_gpt\",\n",
    "        # Key change: Wrap the LiteLLM model identifier\n",
    "        model=LiteLlm(model=MODEL_GPT_4O),\n",
    "        description=\"Provides weather information (using GPT-4o).\",\n",
    "        instruction=\"You are a helpful weather assistant powered by GPT-4o. \"\n",
    "                    \"Use the 'get_weather' tool for city weather requests. \"\n",
    "                    \"Clearly present successful reports or polite error messages based on the tool's output status.\",\n",
    "        tools=[get_weather], # Re-use the same tool\n",
    "    )\n",
    "    print(f\"Agent '{weather_agent_gpt.name}' created using model '{MODEL_GPT_4O}'.\")\n",
    "\n",
    "    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "    session_service_gpt = InMemorySessionService() # Create a dedicated service\n",
    "\n",
    "    # Define constants for identifying the interaction context\n",
    "    APP_NAME_GPT = \"weather_tutorial_app_gpt\" # Unique app name for this test\n",
    "    USER_ID_GPT = \"user_1_gpt\"\n",
    "    SESSION_ID_GPT = \"session_001_gpt\" # Using a fixed ID for simplicity\n",
    "\n",
    "    # Create the specific session where the conversation will happen\n",
    "    session_gpt = await session_service_gpt.create_session(\n",
    "        app_name=APP_NAME_GPT,\n",
    "        user_id=USER_ID_GPT,\n",
    "        session_id=SESSION_ID_GPT\n",
    "    )\n",
    "    print(f\"Session created: App='{APP_NAME_GPT}', User='{USER_ID_GPT}', Session='{SESSION_ID_GPT}'\")\n",
    "\n",
    "    # Create a runner specific to this agent and its session service\n",
    "    runner_gpt = Runner(\n",
    "        agent=weather_agent_gpt,\n",
    "        app_name=APP_NAME_GPT,       # Use the specific app name\n",
    "        session_service=session_service_gpt # Use the specific session service\n",
    "        )\n",
    "    print(f\"Runner created for agent '{runner_gpt.agent.name}'.\")\n",
    "\n",
    "    # --- Test the GPT Agent ---\n",
    "    print(\"\\n--- Testing GPT Agent ---\")\n",
    "    # Ensure call_agent_async uses the correct runner, user_id, session_id\n",
    "    await call_agent_async(query = \"What's the weather in Tokyo?\",\n",
    "                           runner=runner_gpt,\n",
    "                           user_id=USER_ID_GPT,\n",
    "                           session_id=SESSION_ID_GPT)\n",
    "    # --- OR ---\n",
    "\n",
    "    # Uncomment the following lines if running as a standard Python script (.py file):\n",
    "    # import asyncio\n",
    "    # if __name__ == \"__main__\":\n",
    "    #     try:\n",
    "    #         asyncio.run(call_agent_async(query = \"What's the weather in Tokyo?\",\n",
    "    #                      runner=runner_gpt,\n",
    "    #                       user_id=USER_ID_GPT,\n",
    "    #                       session_id=SESSION_ID_GPT)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"An error occurred: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not create or run GPT agent '{MODEL_GPT_4O}'. Check API Key and model name. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a1dc3",
   "metadata": {},
   "source": [
    "Next, we'll do the same for Anthropic's Claude Sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define and Test Claude Agent\n",
    "\n",
    "# Make sure 'get_weather' function from Step 1 is defined in your environment.\n",
    "# Make sure 'call_agent_async' is defined from earlier.\n",
    "\n",
    "# --- Agent using Claude Sonnet ---\n",
    "weather_agent_claude = None # Initialize to None\n",
    "runner_claude = None      # Initialize runner to None\n",
    "\n",
    "try:\n",
    "    weather_agent_claude = Agent(\n",
    "        name=\"weather_agent_claude\",\n",
    "        # Key change: Wrap the LiteLLM model identifier\n",
    "        model=LiteLlm(model=MODEL_CLAUDE_SONNET),\n",
    "        description=\"Provides weather information (using Claude Sonnet).\",\n",
    "        instruction=\"You are a helpful weather assistant powered by Claude Sonnet. \"\n",
    "                    \"Use the 'get_weather' tool for city weather requests. \"\n",
    "                    \"Analyze the tool's dictionary output ('status', 'report'/'error_message'). \"\n",
    "                    \"Clearly present successful reports or polite error messages.\",\n",
    "        tools=[get_weather], # Re-use the same tool\n",
    "    )\n",
    "    print(f\"Agent '{weather_agent_claude.name}' created using model '{MODEL_CLAUDE_SONNET}'.\")\n",
    "\n",
    "    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "    session_service_claude = InMemorySessionService() # Create a dedicated service\n",
    "\n",
    "    # Define constants for identifying the interaction context\n",
    "    APP_NAME_CLAUDE = \"weather_tutorial_app_claude\" # Unique app name\n",
    "    USER_ID_CLAUDE = \"user_1_claude\"\n",
    "    SESSION_ID_CLAUDE = \"session_001_claude\" # Using a fixed ID for simplicity\n",
    "\n",
    "    # Create the specific session where the conversation will happen\n",
    "    session_claude = await session_service_claude.create_session(\n",
    "        app_name=APP_NAME_CLAUDE,\n",
    "        user_id=USER_ID_CLAUDE,\n",
    "        session_id=SESSION_ID_CLAUDE\n",
    "    )\n",
    "    print(f\"Session created: App='{APP_NAME_CLAUDE}', User='{USER_ID_CLAUDE}', Session='{SESSION_ID_CLAUDE}'\")\n",
    "\n",
    "    # Create a runner specific to this agent and its session service\n",
    "    runner_claude = Runner(\n",
    "        agent=weather_agent_claude,\n",
    "        app_name=APP_NAME_CLAUDE,       # Use the specific app name\n",
    "        session_service=session_service_claude # Use the specific session service\n",
    "        )\n",
    "    print(f\"Runner created for agent '{runner_claude.agent.name}'.\")\n",
    "\n",
    "    # --- Test the Claude Agent ---\n",
    "    print(\"\\n--- Testing Claude Agent ---\")\n",
    "    # Ensure call_agent_async uses the correct runner, user_id, session_id\n",
    "    await call_agent_async(query = \"Weather in London please.\",\n",
    "                           runner=runner_claude,\n",
    "                           user_id=USER_ID_CLAUDE,\n",
    "                           session_id=SESSION_ID_CLAUDE)\n",
    "\n",
    "    # --- OR ---\n",
    "\n",
    "    # Uncomment the following lines if running as a standard Python script (.py file):\n",
    "    # import asyncio\n",
    "    # if __name__ == \"__main__\":\n",
    "    #     try:\n",
    "    #         asyncio.run(call_agent_async(query = \"Weather in London please.\",\n",
    "    #                      runner=runner_claude,\n",
    "    #                       user_id=USER_ID_CLAUDE,\n",
    "    #                       session_id=SESSION_ID_CLAUDE)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not create or run Claude agent '{MODEL_CLAUDE_SONNET}'. Check API Key and model name. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd37e72",
   "metadata": {},
   "source": [
    "Observe the output carefully from both code blocks. You should see:\n",
    "\n",
    "Each agent (weather_agent_gpt, weather_agent_claude) is created successfully (if API keys are valid).\n",
    "A dedicated session and runner are set up for each.\n",
    "Each agent correctly identifies the need to use the get_weather tool when processing the query (you'll see the --- Tool: get_weather called... --- log).\n",
    "The underlying tool logic remains identical, always returning our mock data.\n",
    "However, the final textual response generated by each agent might differ slightly in phrasing, tone, or formatting. This is because the instruction prompt is interpreted and executed by different LLMs (GPT-4o vs. Claude Sonnet).\n",
    "This step demonstrates the power and flexibility ADK + LiteLLM provide. You can easily experiment with and deploy agents using various LLMs while keeping your core application logic (tools, fundamental agent structure) consistent.\n",
    "\n",
    "In the next step, we'll move beyond a single agent and build a small team where agents can delegate tasks to each other!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
